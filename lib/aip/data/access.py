""" 
AIP - Data access routines

Rountines for data access. The
get_attacks(start=None, end=None, dates=None, usecols=None)
function should suffice to get the information of the attacks from the models.

Models should invoque the function with a list of dates or a range (start, end).
The function will return a list of Pandas DataFrames ordered by date. An example
the data generated by create_attacks function (from the dataframes are readed)
can be found in the tests/ folder of the project.

This program is free software: you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free Software
Foundation, either version 3 of the License, or (at your option) any later
version.

This program is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
You should have received a copy of the GNU General Public License along with
this program. If not, see <http://www.gnu.org/licenses/>.
"""

__authors__ = ["Joaquin Bogado <joaquinbogado@duck.com>"]
__contact__ = "stratosphere@aic.fel.cvut.cz"
__copyright__ = "Copyright 2022, Stratosphere Laboratory."
__credits__ = ["JoaquÃ­n Bogado"]
__deprecated__ = False
__license__ = "GPLv3"
__maintainer__ = "Joaquin Bogado"
__version__ = "0.0.1"

import datetime as dt
import logging
import pandas as pd
import numpy as np

from aip.data.functions import scramble, read_zeek, getrawdata, removerawdata
from joblib import Parallel, delayed
from os import scandir, path
from pathlib import Path

project_dir = Path(__file__).resolve().parents[3]

data_path = path.join(project_dir,'data')  # Deprecated, do not use
data_dir = path.join(project_dir,'data')

def _get_honeypot_ips(for_date=None):
    '''
    Filter those honeypots active due date for_date, if there are operation dates in the honeypot file.
    '''
    logger = logging.getLogger(__name__)
    # Check if the file exists before attempting to read it
    honeypot_public_ips = path.join(project_dir, 'data', 'external', 'honeypots_public_ips.csv')

    if not path.exists(honeypot_public_ips):
        logger.error(f"File 'honeypot_public_ips.csv' does not exist. Raising error.")
        raise FileNotFoundError("Required file 'honeypots_public_ips.csv' does not exist.")

    honeypots = pd.read_csv(path.join(project_dir, 'data', 'external', 'honeypots_public_ips.csv'), comment='#')
    if for_date is not None:
        for_date = pd.to_datetime(for_date)
        if 'operation_start_date' in honeypots.keys():
            honeypots['operation_start_date'] = pd.to_datetime(honeypots['operation_start_date'])
        if 'operation_end_date' in honeypots.keys():
            honeypots['operation_end_date'] = honeypots['operation_end_date'].fillna(dt.date.today())
            honeypots['operation_end_date'] = pd.to_datetime(honeypots['operation_end_date'])
        if ('operation_start_date' in honeypots.keys()) and 'operation_end_date' in honeypots.keys():
            honeypots = honeypots[(for_date >= honeypots['operation_start_date']) & (for_date <= honeypots['operation_end_date'])]
    ips = honeypots.public_ip.values
    return ips

def _process_zeek_files(zeek_files, date):
    ips = _get_honeypot_ips()
    daily = pd.DataFrame()
    for z in zeek_files:
        hourly = pd.DataFrame()
        zeekdata = read_zeek(z)
        for ip in ips:
            hourly = pd.concat([hourly, zeekdata[zeekdata['id.resp_h'] == ip]])
        daily = pd.concat([daily, hourly])
    return daily

def _process_argus_files(argus_files, date):
    ips = _get_honeypot_ips()
    daily = pd.DataFrame()
    for a in argus_files:
        hourly = pd.DataFrame()
        argusdata = read_argus(a)
        for ip in ips:
            hourly = pd.concat([hourly, argusdata[argusdata['id.resp_h'] == ip]])
        daily = pd.concat([daily, hourly])
    return daily

def _process_raw_files(date):
    '''
    Create a dataset for the date string date in the data/interim folder
    THIS FUNCTION IS DESTRUCTIVE and will overwrite the datasets for the processed date if exists.
    '''
    logger = logging.getLogger(__name__)
    # if data directory does not exist, execute the magic to get it
    if path.isdir(path.join(project_dir,'data','raw', date)) == False:
        logging.debug(f'Downloading data for {date}')
        getrawdata(date)
    # after this point, if directory does not exist, we can skip it.
    try:
        zeek_files = [x.path for x in scandir(path.join(project_dir,'data','raw', date)) if x.name.startswith('conn.')]
    except FileNotFoundError:
        logger.warning(f'Skipping {path.join(project_dir,"data","raw", date)}. Directory not exist.')
        return
    if len(list(zeek_files)) > 0:
        daily = _process_zeek_files(zeek_files, date)
    else:
        daily = pd.DataFrame()
    daily.to_csv(path.join(project_dir,'data','interim', f'daily.conn.{date}.csv.gz'), index=False, compression='gzip')
    logger.debug('Writting file: ' + path.join(project_dir,'data','interim', f'daily.conn.{date}.csv.gz'))
    #logger.debug('Removing raw data (not needed anymore): ' + path.join(project_dir,'data','raw', f'{date}'))
    #removerawdata(date)
    return

def _extract_attacks(date):
    '''
    Create a dataset for the date string date in the data/interim folder
    THIS FUNCTION IS DESTRUCTIVE and will overwrite the datasets for the processed date if exists.
    '''
    logger = logging.getLogger(__name__)
    try:
        daily = pd.read_csv(path.join(project_dir,"data","interim", f'daily.conn.{date}.csv.gz'))
        daily['ts'] = pd.to_datetime(daily['ts'])
        daily['duration'] = daily.duration.replace('-',0).astype(float)
    except FileNotFoundError:
        logger.warning(f'Skipping {path.join(project_dir,"data","interim", f"daily.conn.{date}.csv.gz")}. File not exist.')
        # Generate an empty attacks file
        pd.DataFrame(columns=['orig', 'flows', 'duration', 'packets', 'bytes']).to_csv(
                path.join(project_dir,'data','processed', f'attacks.{date}.csv.gz'), index=False, compression='gzip')
        return
    except pd.errors.EmptyDataError:
        logger.warning(f'Skipping {path.join(project_dir,"data","interim", f"daily.conn.{date}.csv.gz")}. File is empty.')
        # Generate an empty attacks file
        pd.DataFrame(columns=['orig', 'flows', 'duration', 'packets', 'bytes']).to_csv(
                path.join(project_dir,'data','processed', f'attacks.{date}.csv.gz'), index=False, compression='gzip')
        return
    # Calculate the total attacks for each origin
    df = daily[['id.orig_h', 'duration', 'orig_pkts', 'orig_ip_bytes']].groupby(['id.orig_h']).sum()
    df.rename(columns={'duration':'duration', 'orig_pkts':'packets', 'orig_ip_bytes':'bytes'}, inplace=True)
    df['orig'] = df.index.values
    df['flows'] = daily.groupby(['id.orig_h']).count().ts.values
    df.reset_index(drop=True, inplace=True)
    logger.debug('Writting file: ' + path.join(project_dir,'data','processed', f'attacks.{date}.csv.gz'))
    df.to_csv(path.join(project_dir,'data','processed', f'attacks.{date}.csv.gz'), columns=['orig', 'flows', 'duration', 'packets', 'bytes'], index=False, compression='gzip')
    # logger.debug('Removing raw data (not needed anymore): ' + path.join(project_dir,'data','raw', f'{date}'))
    #removerawdata(date)
    return

def process_zeek_files(dates=None):
    """ 
    Creates the dataset or part of it
    """
    logger = logging.getLogger(__name__)
    logger.debug(f'Making  dataset from raw data for dates {dates}')
    if dates is None:
        dates = []
        for x in scandir(path.join(project_dir, 'data', 'raw')):
            try:
                dt.datetime.strptime(x.name, '%Y-%m-%d')
                dates.append(x.name)
            except ValueError:
                pass
    Parallel(n_jobs=12, backend='multiprocessing')(delayed(_process_raw_files)(date) for date in dates)
    return

def extract_attacks(dates=None):
    """
    Creates the dataset or part of it
    """
    logger = logging.getLogger(__name__)
    logger.debug(f'Creating attacks for dates {dates}')
    filesready = [x.name for x in scandir(path.join(project_dir, 'data', 'processed'))]
    datesnotready = []
    for date in dates:
        if f'daily.conn.{date}.csv.gz' not in filesready:
            datesnotready.append(date)
    process_zeek_files(datesnotready)
    if dates is None:
        dates = []
        for x in scandir(path.join(project_dir, 'data', 'interim')):
            try:
                dt.datetime.strptime(x.name, '%Y-%m-%d')
                dates.append(x.name)
            except ValueError:
                pass
    Parallel(n_jobs=12, backend='multiprocessing')(delayed(_extract_attacks)(date) for date in dates)
    return

def get_attacks(start=None, end=None, dates=None, usecols=None):
    '''
    Returns a DataFrame with the attacks between the dates start and end or the
    ones especified in the list dates.
    '''
    if start is not None:
        dates = [str(x.date()) for x in pd.date_range(start, end)]
    
    filesready = [x.name for x in scandir(path.join(project_dir, 'data', 'processed'))]
    datesnotready = []
    for date in dates:
        if f'attacks.{date}.csv.gz' not in filesready:
            datesnotready.append(str(date))
    if len(datesnotready) > 0:
        extract_attacks(datesnotready)
    dfs = [pd.read_csv(path.join(project_dir, 'data', 'processed',f'attacks.{date}.csv.gz'), usecols=usecols, comment='#')
            for date in dates]
    return dfs

if __name__ == '__main__':
    log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    #logging.basicConfig(level=logging.INFO, format=log_fmt)
    logging.basicConfig(level=logging.DEBUG, format=log_fmt)

    # find .env automagically by walking up directories until it's found, then
    # load up the .env entries as environment variables
    #load_dotenv(find_dotenv())

    extract_attacks()

